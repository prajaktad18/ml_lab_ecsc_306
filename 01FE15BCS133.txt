USN : 01FE15BCS133
GIT Username : prajaktad18
BATCH : C1

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

LAB 1 : ASSIGNMNET 2 :
#----------------------------------------------- PART 1 ------------------------------------------------------
import tensorflow as tf

sess = tf.Session()
#1.Read arrays x and y containing floating numbers
array_x = tf.constant([1.0,2.0,3.0], name = 'x')
array_y = tf.constant([2.0,3.0,4.0], name = 'y')

#2. Calculate mean of x and y
mean_x = tf.reduce_mean(array_x)
mean_y = tf.reduce_mean(array_y)

#3. Calucate variance of x
#variance(x)=sum((x-mean(x))^2)
with tf.name_scope("Variance_X"):
    sq_x = tf.square(array_x - mean_x)

    add_x = tf.reduce_sum(sq_x)

    #get size of the array
    n = (array_x.get_shape())
    n = tf.to_float(n)
    #print(n)

    variance_x = tf.div(add_x,n,name="Variance_x")
    
    #writer = tf.summary.FileWriter("/tmp/tboard/l1a2_pa", sess.graph)
    print(sess.run(variance_x))
    #writer.close()
    
#4. Calculate covariance of x and y
#covariance = sum((x(i) - mean(x)) * (y(i) - mean(y)))

with tf.name_scope("Covariance"):
    co_x_y=0.0
    
    for i in range(len(sess.run(array_x))):
        co_x_y = tf.add(co_x_y,tf.multiply(tf.subtract(array_x[i],mean_x),tf.subtract(array_y[i],mean_y)))
        
#5. m=covariance(x,y)/variance(x) 
with tf.name_scope("m"):
    m = tf.div(co_x_y,variance_x,name="m")

#6. c=mean(y)-m*mean(x)
with tf.name_scope("c"):
    c = tf.subtract(mean_y,tf.multiply(m,mean_x))
    
# y = mx + c
with tf.name_scope("Linear_model"):
    y = tf.add(tf.multiply(m,array_x),c)
    
#Printing the values
print("Mean of x = ",sess.run(mean_x))
print("Mean of y = ",sess.run(mean_y))
print("Variance of x = ",sess.run(variance_x))
print("Covariance(x,y) = ",sess.run(co_x_y))
print("Slope,m = ",sess.run(m))
print("Constant,c = ",sess.run(c))   
print("y = mx + c = ",sess.run(y))

#----------------------------------------------- PART 2 -----------------------------------------------------
import matplotlib.pyplot  as plt

#1. Plot graph for actual values against predicted value
print("Array y : ",sess.run(array_y))
print("Predicted y : ",sess.run(y))

#plotting the graph
plt.plot(sess.run(array_y),sess.run(y))
#plt.axis(0,7,0,8)
plt.show()

#2. Calculate root mean square error.
rmse = tf.sqrt(tf.reduce_sum(tf.pow(y-array_y, 2))/(len(sess.run(array_x))))
print("Root mean square error:",sess.run(rmse))

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

LAB 2 : ASSIGNMENT 2 :

import tensorflow as tf
import numpy
import matplotlib.pyplot as plt
rng = numpy.random

learning_rate = 0.005
training_epochs = 1000
display_step = 50

# Training Data
train_X = numpy.asarray([1,2,4,3,5])
train_Y = numpy.asarray([1,3,3,2,5])
n_samples = train_X.shape[0]

# tf Graph Input
X = tf.placeholder("float")
Y = tf.placeholder("float")

# Set model weights
W = tf.Variable(rng.randn(), name="weight")
b = tf.Variable(rng.randn(), name="bias")

# Construct a linear model
pred = tf.add(tf.multiply(X, W), b)

# Mean squared error
cost = tf.reduce_sum(tf.pow(pred-Y, 2))/(2*n_samples)
# Gradient descent
optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)

# Initializing the variables
init = tf.global_variables_initializer()

# Launch the graph
with tf.Session() as sess:
    sess.run(init)
   # print(sess.run(cost))
    # Fit all training data
    for epoch in range(100):
        for (x, y) in zip(train_X, train_Y):
            sess.run(optimizer, feed_dict={X: x, Y: y})

        #Display logs per epoch step
        if (epoch+1) % display_step == 0:
            c = sess.run(cost, feed_dict={X: train_X, Y:train_Y})
            print ("Epoch:", '%04d' % (epoch+1), "cost=", "{:.9f}".format(c), \
                "W=", sess.run(W), "b=", sess.run(b))

    print ("Optimization Finished!")
    training_cost = sess.run(cost, feed_dict={X: train_X, Y: train_Y})
    print ("Training cost=", training_cost, "W=", sess.run(W), "b=", sess.run(b), '\n')
    plt.plot(train_X, train_Y, 'ro', label='Original data')
    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line')
    plt.legend()
    plt.show()

    
    optimizer = tf.train.GradientDescentOptimizer(0.005).minimize(cost)
    # Launch the graph
with tf.Session() as sess:
    sess.run(init)

    # Fit all training data
    for epoch in range(100):
        for (x, y) in zip(train_X, train_Y):
            sess.run(optimizer, feed_dict={X: x, Y: y})

        #Display logs per epoch step
        if (epoch+1) % display_step == 0:
            c = sess.run(cost, feed_dict={X: train_X, Y:train_Y})
            print ("Epoch:", '%04d' % (epoch+1), "cost=", "{:.9f}".format(c), \
                "W=", sess.run(W), "b=", sess.run(b))

    print ("Optimization Finished!")
    training_cost = sess.run(cost, feed_dict={X: train_X, Y: train_Y})
    print ("Training cost=", training_cost, "W=", sess.run(W), "b=", sess.run(b), '\n')
    
    #Graphic display
    plt.plot(train_X, train_Y, 'ro', label='Original data')
    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line')
    plt.legend()
    plt.show()

OUTPUT : 
#1
Epoch: 0050 cost= 0.246302292 W= 0.87786 b= 0.144511
Epoch: 0100 cost= 0.245549560 W= 0.869166 b= 0.153372
Optimization Finished!
Training cost= 0.24555 W= 0.869166 b= 0.153372 

#2
Epoch: 0050 cost= 0.246302292 W= 0.87786 b= 0.144511
Epoch: 0100 cost= 0.245549560 W= 0.869166 b= 0.153372
Optimization Finished!
Training cost= 0.24555 W= 0.869166 b= 0.153372 

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
